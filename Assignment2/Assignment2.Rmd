---
title: "Assignment II"
author: "Group2"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("pacman")
pacman::p_load(tidyverse,rms,haven,mgcv,epitools,logistf,nlpred,geepack,skimr,pROC,tableone,emmeans,glmtoolbox,CalibrationCurves,mice, dcurves)
library(tableone)
```

# Introduction

---

# Question 1:

Create table 1. Describe all available variables in the data. Show both, the original data and the imputed data. 

## Overview: 

All available variables in the data are as follows:

- *agegroup:* Categories for age.
- *smoker:*  Binary variable for smoker or not.
- *smokerf:*  Smoker factor with levels No and Yes.
- *heightcm:* Convert height from inches to cm.
- *weightkg:* Convert weight from pounds to kg.
- *bmi:* Calculate BMI.
- *bmicat:* Categories for BMI.
- *cholmmol:* Convert cholesterol from mg/dl to mmol/l.
- *sbp10:*  Categories of sbp (systolic blood pressure).
- *sbpcat:* Systolic blood pressure factor.
- *dibpat0f:* Dichotomous behavior pattern factor with levels A and B instead of 1 and 0. A classification system where individuals are grouped into one of two distinct categories based on their behavioral traits
- *arcus0:* Corneal arcus factor which is caused by lipid deposits in the cornea. It's presence may indicate high cholesterol levels and increased risk of heart disease.
- *chd69f:* Coronary heart disease factor.


Then, we have created tables for the original data. In the next step, we have imputed the data using Multivariate Imputation and created tables for the imputed data.

## Code:

```{r echo=FALSE}
# read in the data
data(wcgs)
# create factors from var ditpat0 with levels B and A in stead of 0 and 1 
wcgs$dibpat0f<-factor(wcgs$dibpat0,levels=0:1,label=c("B","A"))
# create categories for age
wcgs$agegroup <- cut(wcgs$age0,breaks=c(39,45,55,60),include.lowest = T,right = FALSE) 
# binary variable for smoker or not
wcgs$smoker <- ifelse(wcgs$ncigs0>0,1,0)
wcgs$smokerf <- factor(wcgs$smoker,levels=c(0,1),labels=c("No","Yes"))
# convert height from inches to cm
wcgs$heightcm <- wcgs$height0*2.54
wcgs$weightkg <- wcgs$weight0*0.45359237
wcgs$bmi <- wcgs$weightkg / (wcgs$heightcm/100)^2
wcgs$bmicat <- cut(wcgs$bmi,breaks=c(0,25,30,40),include.lowest = T,right = FALSE)
wcgs$cholmmol <- wcgs$chol0/39
wcgs$sbp10 <- wcgs$sbp0/10
# Create categories of sbp (systolic blood pressure). Make sure to have the lowest and highest numbers
wcgs$sbpcat <- cut(wcgs$sbp0,breaks=c(0,140,240),include.lowest = T,right = FALSE) # For use when tabulating the data you can create labels
wcgs$chd69f <- factor(wcgs$chd69,levels=c(0,1),labels=c("No","Yes")) 
wcgs$cholmmol <- ifelse(wcgs$cholmmol<15,wcgs$cholmmol,NA)

d <- wcgs %>% select(id,agegroup,age0,cholmmol,sbp10,bmi,smokerf,arcus0,dibpat0f,chd69)
dc <- d %>% drop_na()


set.seed(154550)
imp <- mice(d, m=1, maxit=0)
predM<-imp$predictorMatrix
# Leave out the ID column (the first column)
predM[, 1] <- 0
meth<-imp$method
dimp <- mice(d, method= "pmm" ,m=1,predictorMatrix = predM, maxit=15, seed=71332, print=FALSE)
di <- complete(dimp,1)
             
#di[is.na(d$cholmmol),"cholmmol"]
```

```{r}
# Define variables
variables <- c("id", "agegroup", "age0", "cholmmol", "sbp10", "bmi", "smokerf",
               "arcus0", "dibpat0f", "chd69")
categorical <- c("smokerf", "dibpat0f", "chd69")

# Create Table 1 for the original data
table_original <- CreateTableOne(vars = variables, data = wcgs, factorVars = categorical)

# Create Table 1 for the imputed data
table_imputed <- CreateTableOne(vars = variables, data = di, factorVars = categorical)
```

## Output: 

```{r echo=FALSE}
# Print summary for comparison
summary(table_original)
summary(table_imputed)
```

## Conclusion: 

The imputed data has been created using Multivariate Imputation where the missing data of cholmmol has been imputed. The imputed data has been created using Predictive Mean Matching (PMM) method. 

# Question 2:

Calculate the overall risk of CHD in the cohort.

## Overview: 

**a. What is the outcome we are interested in?**

The outcome we are interested in is Coronary Heart Disease (CHD).

**b. What are the known risk factors for our outcome of interest?**

The known risk factors for Coronary Heart Disease (CHD) are as follows:

- Dichotomous Behaviour type A/B (dibpat0f)
- Age (agegroup, age0)
- Cholesterol (cholmmol)
- Systolic Blood Pressure (sbp10)
- BMI (bmi)
- Smoking (smokerf)
- Corneal arcus (arcus0)

**c. How many persons are included?**

3154 middle-aged men, from 39 to 59 years of age, during the years 1960-1961 are included in this prospective cohort study.

**d. What is the overall risk or rate and prevalence of the disease in our cohort?**

The overall risk or rate and prevalence of the disease in our cohort is as follows:

```{r}
# Overall risk or rate
overall_rate <- table(di$chd69)

#calculate risk of CHD
overall_risk <- overall_rate / sum(overall_rate)

# extract the rate and risk into a data frame
chd_frame <- data.frame(
  "CHD Presence" = c("No", "Yes"),
  "Overall Rate" = c(as.matrix(overall_rate)[1], as.matrix(overall_rate)[2]),
  "Overall Risk" = c(as.matrix(overall_risk)[1], as.matrix(overall_risk)[2])
)
#print overall rate and risk into a table
knitr::kable(chd_frame, col.names = c("CHD Presence", "Overall Rate", "Overall Risk"))

```
                                 
## Analysis: 

The overall risk of Coronary Heart Disease (CHD) in the cohort is 0.08, which indicates that prevalence of the disease is 8% in the cohort.

# Question 3:

## Overview: 

To solve this problem, we need to build an optimal prediction model for the outcome of Coronary Heart Disease (CHD)
using the available data. We will use logistic regression due to the binary nature of the outcome and select predictors that improve our predictions. Additionally, we will consider interaction terms and ensure that categorical variables are appropriately handled.

#### 3.a. Building the Optimal Prediction Model:\newline

**Step 1: Model Selection:**

Logistic regression is suitable for predicting Coronary Heart Disease (CHD) because:

- Binary outcome: CHD is a binary outcome, meaning it can be either present (1) or absent (0). Logistic regression is designed to model binary outcomes.
- Multiple predictors: There are multiple known risk factors for CHD, and logistic regression can handle multiple predictor variables.
- Quantification of risk: Logistic regression can provide estimates of the probability of developing CHD based on the values of the predictor variables, which can be useful for risk assessment and decision-making. 

The `rms` package in R provides functions for regression modeling strategies, including logistic regression via the `lrm` function.

**Step 2: Variable Selection:**

We start by fitting a full model that includes all potential predictors:

```{r, warning=FALSE}
dd <- datadist(di)
options(datadist="dd")
full_model <- lrm(chd69 ~ dibpat0f + age0 + cholmmol + sbp10 + bmi + smokerf 
                  + arcus0, data=di, x=TRUE, y=TRUE)
# Extract the model summary
model_summary <- as.data.frame(summary(full_model))
knitr::kable(model_summary, col.names = c("Variable", "Low", "High", "Diff", "Effect",
                                          "S.E.", "Lower 95%", "Upper 95%"), 
             align = c("l", "c", "c", "c", "c", "c", "c", "c"), 
             caption = "Summary of the Logistic Regression Model")
AIC(full_model)
```

#### 3.b. Including Interaction Terms\newline

We need to check if including interaction terms between certain predictors improves the model fit. When considering interaction terms in a logistic regression model, one needs to think which variables might have a combined effect on the outcome (Coronary Heart Disease) that's different from their individual effects. Here are some potential interaction terms along with their rationale:

- **Age and Cholesterol:** As people age, their cholesterol levels may have a greater impact on their risk of Coronary Heart Disease. This interaction term can help capture the potential synergistic effect of increasing age and cholesterol levels.
- **Smoking and Age:** Smoking is a well-known risk factor for Coronary Heart Disease, and its effects may be exacerbated with increasing age. This interaction term can help account for the potential increased risk of Coronary Heart Disease among older smokers.
- **BMI and Systolic Blood Pressure:** High blood pressure is often associated with obesity, and the combination of these two factors may increase the risk of Coronary Heart Disease more than either factor alone. This interaction term can help capture the potential additive effect of high BMI and systolic blood pressure.
- **Cholesterol and Systolic Blood Pressure:** High cholesterol and high blood pressure are both risk factors for Coronary Heart Disease, and their combined effect may be greater than the sum of their individual effects. This interaction term can help account for the potential synergistic effect of these two factors.
- **Corneal arcus and Age:** Corneal arcus is a sign of lipid deposition in the cornea, which may be associated with increased risk of Coronary Heart Disease. The effect of corneal arcus may be more pronounced in older individuals, making this interaction term a potential candidate.
- **Smoking and Cholesterol:** Smoking can increase cholesterol levels, and the combination of these two factors may increase the risk of Coronary Heart Disease more than either factor alone. This interaction term can help capture the potential additive effect of smoking and high cholesterol.
- **Age and BMI:** As people age, their BMI may have a greater impact on their risk of Coronary Heart Disease. This interaction term can help account for the potential increased risk of Coronary Heart Disease among older individuals with high BMI.
- **Cholesterol and Dichotomous Behaviour type:** The Type A behaviour type is historically linked to increased risk of heart disease. This interaction term can help capture the potential additive effect of high cholesterol and Type A behaviour type.
- **BMI and Dichotomous Behaviour type:** Type A behaviour type is associated with stress and may interact with BMI to increase the risk of Coronary Heart Disease. This interaction term can help account for the potential combined effect of high BMI and Type A behaviour type.

We first define a base formula and then consider various interaction terms to see if they improve the model fit. After fitting the models, we compare them based on their AIC values to select the best model.

### Code:

```{r}
# Define the base formula
base_formula <- as.formula("chd69 ~ dibpat0f + age0 + cholmmol + sbp10 + bmi + smokerf + arcus0")
# Define potential interaction terms
interaction_terms <- c("age0*cholmmol", "age0:smokerf", "bmi * sbp10", 
                       "cholmmol*sbp10", "age0*arcus0 ", "cholmmol:dibpat0f", 
                       "smokerf * cholmmol", "age0 * bmi", "sbp10:smokerf", 
                       "bmi:dibpat0f")

# Initialize a list to store models and metrics
models <- list()
metrics <- data.frame(Model = character(), AIC = numeric(), stringsAsFactors =
                        FALSE)

# Loop through interaction terms
for (i in 1:length(interaction_terms)) {
  for (j in combn(interaction_terms, i, simplify = FALSE)) {
    # Create formula with interactions
    interaction_formula <- paste(base_formula, paste(j, collapse = " + "),
                                 sep = " + ")
    full_formula <- as.formula(interaction_formula)
    
    # Fit the model
    model <- lrm(full_formula, data = di, x = TRUE, y = TRUE)
    
    # Save the model and its AIC
    models[[paste(j, collapse = ", ")]] <- model
    metrics <- rbind(metrics, data.frame(Model = paste(j, collapse = ", "),
                                         AIC = AIC(model)))
  }
}

# Sort models by AIC
metrics <- metrics[order(metrics$AIC), ]

# View the first 15 top-performing models
knitr::kable(head(metrics, 15), col.names = c("Model", "AIC"))

```
**Explanation**

After exploring various models with various combinations of interaction terms along with the full model, we went through a model selection process using AIC to compare the goodness-of-fit. We ultimately chose the model including the interactions between `bmi` and `sbp10`, and between `age0` and `arcus0` as it gave the lowest AIC on comparing with every other model combination.

- **Interaction Terms**: Interaction terms allow us to assess whether the effect of one predictor on the outcome depends on the level of another predictor. For example, the effect of BMI on CHD might vary depending on systolic blood pressure.
- **Model Comparison**: The likelihood ratio test helps determine if the addition of interaction terms significantly improves the model fit. Since the p-value is significant (p < 0.05), we include the interaction terms; otherwise, we would have retained the model without interactions. Coupled with the fact that the AIC was slightly better than the model without interactions though it adds a certain level of complexity given the additional number of parameters

```{r}
# Compare using LR test
final_model <- models[[29]]
lrtest(full_model, final_model)
```

#### 3.c. Calculating Predicted Risks

Once the final model is selected, we calculate the predicted probabilities of CHD for each individual in the dataset and add these predictions to the dataset.

**Predicted Risks**: These probabilities provide an estimate of each individual's risk of developing CHD based on the predictor values in the model. This information can be crucial for further analysis, such as assessing model calibration or making risk-based decisions.

```{r, warning=FALSE}
di$predicted_risk <- predict(final_model, di, type="fitted")
head(di$predicted_risk)

```

These values can be compared to the previously calculated overall risk of 0.0814838 in the cohort to see how individual risks vary based on the predictor variables.

## Conclusion:

1. **Model Selection and Variable Selection**:
   - Started with a full logistic regression model.

2. **Interaction Terms**:
   - Assessed interaction effects between various predictors based on domain knowledge. Considered multiple interaction terms to improve model fit.
   - Used likelihood ratio test to compare models with and without interactions.

3. **Predicted Risks**:
   - Calculated predicted probabilities of CHD for each individual and added them to the dataset.

This approach ensures that the final model is both statistically sound and practically useful for predicting CHD risk.

# Question 4

## Overview : 

## Code :

```{r echo=FALSE}

```

## Output : 

## Analysis : 

# Question 5

## a

### Overview : 

To evaluate the model’s calibration, we will plot the calibration curve using the rms package in R. This curve compares the predicted probabilities from the logistic regression model with the observed outcomes. The slope and intercept of the calibration curve will also be reported, reflecting how well the model’s predictions align with the observed data.

### Code :

```{r echo=FALSE}
# Plot the calibration curve
cal <- calibrate(full_model, method = "boot", B = 1000)  
plot(cal, main = "Calibration Curve")

# Extract calibration points
cal_points <- as.data.frame(cal[, c("predy", "calibrated.corrected")])  
names(cal_points) <- c("Predicted", "Observed")

# Fit the linear regression model of the calibration points and calculate the slope and intercept
cal_model <- lm(Observed ~ Predicted, data = cal_points)
calibration_slope <- coef(cal_model)["Predicted"]
calibration_intercept <- coef(cal_model)["(Intercept)"]

# Report the slope and the intercept of the calibration curve
cat("Calibration Slope:", calibration_slope, "\n")
cat("Calibration Intercept:", calibration_intercept, "\n")
```

### Output : 

A calibration curve and the slope as well as the intercept of the calibration curve.

### Analysis : 

From the calibration curve, it can be observed that there are deviations between the solid line and the reference line, indicating potential miscalibration. When the curve lies below the reference line, the model tends to underestimate probabilities. When it lies above, the model tends to overestimate probabilities. The bottom text shows the mean absolute error (0.008), which quantifies the average deviation between predicted and observed probabilities. Lastly, the slope of the calibration curve is 0.6127853, and the intercept of the calibration is 0.04989182. 

## b

### Overview : 

The Hosmer-Lemeshow test will be applied to assess the goodness of fit of the logistic regression model. This test divides the data into deciles based on predicted probabilities and evaluates whether the observed outcomes match the predicted values within these groups. The test statistic, degrees of freedom, and p-value will be interpreted to determine model fit.

### Code :

```{r echo=FALSE}
library(ResourceSelection)

# Perform Hosmer-Lemeshow test on the model
hl_test = hoslem.test(di$chd69, di$predicted_risk)
hl_test

cat("Hosmer-Lemeshow Test: Chi-squared =", hl_test$statistic, 
    ", df =", hl_test$parameter, 
    ", p-value =", hl_test$p.value, "\n")
```

### Output : 

The result from the Hosmer-Lemeshow test.

### Analysis : 

According to the result from the Hosmer-Lemeshow test, the p-value is larger than 0.05, which means that we cannot reject the null hypothesis that the model's predicted probabilities are well-calibrated and fit the observed data.

## c

### Overview : 

A new logistic regression model will be created using only the variable agegroup as the predictor. The discrimination ability of this simplified model will be estimated using the Area Under the Curve (AUC), a measure of the model’s ability to differentiate between positive and negative outcomes.

### Code :

```{r echo=FALSE}
# Assuming 'agegroup' is a variable in the dataset
agegroup_model <- lrm(chd69 ~ agegroup, data = di)

# Predict probabilities for the agegroup model
agegroup_probs <- predict(agegroup_model, data = di, type = "fitted")

# Compute the ROC curve and AUC for the agegroup model
roc_agegroup <- roc(di$chd69, agegroup_probs)
auc(roc_agegroup)
```

### Output : 

The AUC value of the new Agegroup model.

### Analysis : 

According to the calculation result of the AUC value, the AUC value is 0.6063. The value indicates this Agegroup model can provide some useful predictions, but its discriminative power is not strong enough for high-confidence decision-making.

## d

### Overview : 

The discrimination performance of the original model and the simplified agegroup-only model will be compared statistically. The DeLong test will be employed to determine if the difference in the AUCs of the two models is statistically significant. The test result will help assess whether including additional predictors improves the model’s performance.

### Code :

```{r echo=FALSE}
# Assuming 'model_probs' are the predicted probabilities from the final model
roc_final_model <- roc(di$chd69, di$predicted_risk)

# DeLong test for comparing AUCs
roc_comparison <- roc.test(roc_final_model, roc_agegroup)
roc_comparison
```

### Output : 

The result from DeLong's test for two correlated ROC curves.

### Analysis : 

From the result of DeLong's test for two correlated ROC curves, the AUC of the roc from the final model is 0.7516887, and the AUC of the roc from the new Agegroup model is 0.6063021. Moreover, the p-value is less than 0.05, which indicates that the AUC of the roc from the final model is significantly higher than the AUC of the roc from the new Agegroup model.

## e

### Overview : 

The Receiver Operating Characteristic (ROC) curves for both the original model and the simplified agegroup-only model will be plotted on the same graph. This visualization will allow for a direct comparison of the two models' discrimination capabilities. Key features such as AUC values and curve shapes will be highlighted.

### Code :

```{r echo=FALSE}
# Plot both ROC curves together
plot(roc_final_model, col = "blue", main = "ROC Curves Comparison", lwd = 2)
lines(roc_agegroup, col = "red", lwd = 2)

# Add a legend
legend("bottomright", legend = c("Final Model", "Agegroup Model"),
       col = c("blue", "red"), lwd = 2)
```

### Output : 

Both ROC curves are in one figure.

### Analysis : 

Both ROC curves are plotted in one figure as above. This figure further justifies our conclusion that the AUC of the roc from the final model is significantly higher than the AUC of the roc from the new Agegroup model.

# Question 6

## Overview : 

## Code :

```{r echo=FALSE}

```

## Output : 

## Analysis : 

# Question 7

## Overview : 

## Code :

```{r echo=FALSE}

```

## Output : 

## Analysis : 

# Question 8

## Overview : 

## Code :

```{r echo=FALSE}

```

## Output : 

## Analysis : 

# Question 9

## Overview : 

## Code :

```{r echo=FALSE}

```

## Output : 

## Analysis : 

# Question 10

## Overview : 

## Code :

```{r echo=FALSE}

```

## Output : 

## Analysis : 

# Question 11

## Overview : 

## Code :

```{r echo=FALSE}

```

## Output : 

## Analysis : 

# Question 12

## Overview : 

## Code :

```{r echo=FALSE}

```

## Output : 

## Analysis : 

# Question 13

## Overview : 

## Code :

```{r echo=FALSE}

```

## Output : 

## Analysis : 
